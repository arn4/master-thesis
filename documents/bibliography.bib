%% Introduction machine learning/ statistical physics
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{sarao2019afraid,
  title={Who is afraid of big bad minima? analysis of gradient-flow in spiked matrix-tensor models},
  author={Sarao Mannelli, Stefano and Biroli, Giulio and Cammarota, Chiara and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{kirkpatrick1983optimization,
  title={Optimization by simulated annealing},
  author={Kirkpatrick, Scott and Gelatt Jr, C Daniel and Vecchi, Mario P},
  journal={science},
  volume={220},
  number={4598},
  pages={671--680},
  year={1983},
  publisher={American association for the advancement of science}
}

@article{hopfield1982neural,
  title={Neural networks and physical systems with emergent collective computational abilities.},
  author={Hopfield, John J},
  journal={Proceedings of the national academy of sciences},
  volume={79},
  number={8},
  pages={2554--2558},
  year={1982},
  publisher={National Acad Sciences}
}

@article{gabrie18training,
  title={Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer Free Energy},
  author={Gabri{\'e}, Marylou and Tramel, Eric W and Krzakala, Florent},
  journal={representations},
  volume={18},
  number={19},
  pages={19}
}

@article{gabrie2020mean,
  title={Mean-field inference methods for neural networks},
  author={Gabri{\'e}, Marylou},
  journal={Journal of Physics A: Mathematical and Theoretical},
  volume={53},
  number={22},
  pages={223002},
  year={2020},
  publisher={IOP Publishing}
}

%% Online soft committee machine
@article{veiga2022phase,
  title={Phase diagram of Stochastic Gradient Descent in high-dimensional two-layer neural networks},
  author={Veiga, Rodrigo and Stephan, Ludovic and Loureiro, Bruno and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={arXiv e-prints},
  pages={arXiv--2202},
  year={2022}
}

@article{aubin2018committee,
  title={The committee machine: Computational to statistical gaps in learning a two-layers neural network},
  author={Aubin, Benjamin and Maillard, Antoine and Krzakala, Florent and Macris, Nicolas and Zdeborov{\'a}, Lenka and others},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{saad1995line,
  title={On-line learning in soft committee machines},
  author={Saad, David and Solla, Sara A},
  journal={Physical Review E},
  volume={52},
  number={4},
  pages={4225},
  year={1995},
  publisher={APS}
}

@article{goldt2019dynamics,
  title={Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup},
  author={Goldt, Sebastian and Advani, Madhu and Saxe, Andrew M and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{wang2017scaling,
  title={Scaling limit: Exact and tractable analysis of online learning algorithms with applications to regularized regression and PCA},
  author={Wang, Chuang and Mattingly, Jonathan and Lu, Yue M},
  journal={arXiv preprint arXiv:1712.04332},
  year={2017}
}

@article{wang2019solvable,
  title={A solvable high-dimensional model of GAN},
  author={Wang, Chuang and Hu, Hong and Lu, Yue},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{refinetti2022dynamics,
  title={The dynamics of representation learning in shallow, non-linear autoencoders},
  author={Refinetti, Maria and Goldt, Sebastian},
  journal={arXiv preprint arXiv:2201.02115},
  year={2022}
}

@article{chizat2019lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


%% Generic Neural Networks
@incollection{szandala2021review,
  title={Review and comparison of commonly used activation functions for deep neural networks},
  author={Szanda{\l}a, Tomasz},
  booktitle={Bio-inspired neurocomputing},
  pages={203--224},
  year={2021},
  publisher={Springer}
}

@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}

% SDE
@book{morters2010brownian,
  title={Brownian motion},
  author={M{\"o}rters, Peter and Peres, Yuval},
  volume={30},
  year={2010},
  publisher={Cambridge University Press}
}

@online{smith2018ito,
  author = {Smith, Lewis},
  title = {Itô and Stratonovich; a guide for the perplexed},
  year = 2018,
  url = {https://www.robots.ox.ac.uk/~lsgs/posts/2018-09-30-ito-strat.html},
  note = "[Online; accessed 16-August-2022]"
}

@article{zeng2020mean,
  title={Mean exit time and escape probability for the Ornstein--Uhlenbeck process},
  author={Zeng, Caibin},
  journal={Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume={30},
  number={9},
  pages={093127},
  year={2020},
  publisher={AIP Publishing LLC}
}

@misc{wiki2022euler,
  author = "{Wikipedia contributors}",
  title = "Euler–Maruyama method --- {Wikipedia}{,} The Free Encyclopedia",
  year = "2022",
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Euler–Maruyama_method&oldid=1104040790}",
  note = "[Online; accessed 24-August-2022]"
}

%% Isserlis 
@article{withers1985moments,
  title={The moments of the multivariate normal},
  author={Withers, Christopher S},
  journal={Bulletin of the Australian Mathematical Society},
  volume={32},
  number={1},
  pages={103--107},
  year={1985},
  publisher={Cambridge University Press}
}

%% External package
@incollection{pytorch2019, 
  title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith}, 
  booktitle = {Advances in Neural Information Processing Systems 32}, 
  pages = {8024--8035}, 
  year = {2019}, 
  publisher = {Curran Associates, Inc.}, 
  url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf} 
}

%% SPherical stuff
@article{chizat2018global,
  title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
  author={Chizat, Lenaic and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

%% Phase retrivial
@article{mignacco2021stochasticity,
  title={Stochasticity helps to navigate rough landscapes: comparing gradient-descent-based algorithms in the phase retrieval problem},
  author={Mignacco, Francesca and Urbani, Pierfrancesco and Zdeborov{\'a}, Lenka},
  journal={Machine Learning: Science and Technology},
  volume={2},
  number={3},
  pages={035029},
  year={2021},
  publisher={IOP Publishing}
}

@article{maillard2020phase,
  title={Phase retrieval in high dimensions: Statistical and computational phase transitions},
  author={Maillard, Antoine and Loureiro, Bruno and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={11071--11082},
  year={2020}
}

@inproceedings{mondelli2018fundamental,
  title={Fundamental limits of weak recovery with applications to phase retrieval},
  author={Mondelli, Marco and Montanari, Andrea},
  booktitle={Conference On Learning Theory},
  pages={1445--1450},
  year={2018},
  organization={PMLR}
}

@article{sarao2020complex,
  title={Complex dynamics in simple neural networks: Understanding gradient flow in phase retrieval},
  author={Sarao Mannelli, Stefano and Biroli, Giulio and Cammarota, Chiara and Krzakala, Florent and Urbani, Pierfrancesco and Zdeborov{\'a}, Lenka},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3265--3274},
  year={2020}
}

@article{sarao2020optimization,
  title={Optimization and generalization of shallow neural networks with quadratic activation functions},
  author={Sarao Mannelli, Stefano and Vanden-Eijnden, Eric and Zdeborov{\'a}, Lenka},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13445--13455},
  year={2020}
}

@article{arous2021online,
  title={Online stochastic gradient descent on non-convex losses from high-dimensional inference.},
  author={Arous, Gerard Ben and Gheissari, Reza and Jagannath, Aukosh},
  journal={J. Mach. Learn. Res.},
  volume={22},
  pages={106--1},
  year={2021}
}

@article{tan2019online,
  title={Online stochastic gradient descent with arbitrary initialization solves non-smooth, non-convex phase retrieval},
  author={Tan, Yan Shuo and Vershynin, Roman},
  journal={arXiv preprint arXiv:1910.12837},
  year={2019}
}

%% Random
@misc{enwiki:halfnormaldistribution,
  author = "{Wikipedia contributors}",
  title = "Half-normal distribution --- {Wikipedia}{,} The Free Encyclopedia",
  year = "2022",
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Half-normal_distribution&oldid=1089937121}",
  note = "[Online; accessed 1-August-2022]"
}

% Ben Arous Brownian
@article{arous2022high,
  title={High-dimensional limit theorems for SGD: Effective dynamics and critical scaling},
  author={Arous, Gerard Ben and Gheissari, Reza and Jagannath, Aukosh},
  journal={arXiv preprint arXiv:2206.04030},
  year={2022}
}

% Future 
@article{mei2018mean,
  title={A mean field view of the landscape of two-layer neural networks},
  author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={33},
  pages={E7665--E7671},
  year={2018},
  publisher={National Acad Sciences}
}

@article{rotskoff2018trainability,
  title={Trainability and accuracy of neural networks: An interacting particle system approach},
  author={Rotskoff, Grant M and Vanden-Eijnden, Eric},
  journal={arXiv preprint arXiv:1805.00915},
  year={2018}
}

@article{sirignano2020mean,
  title={Mean field analysis of neural networks: A central limit theorem},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={Stochastic Processes and their Applications},
  volume={130},
  number={3},
  pages={1820--1852},
  year={2020},
  publisher={Elsevier}
}